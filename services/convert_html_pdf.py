# -*- coding: utf-8 -*-
"""
convert_html_pdf.py
- Streamlit app: convert ì§€ì¶œê²°ì˜ì„œ HTML -> CSV
- Saves CSV to data/vectorstore/converted as: "{ì‚¬ìš©ì}_ì§€ì¶œê²°ì˜ì„œ_converted.csv"
"""

import io
from pathlib import Path
from typing import Dict, List, Optional

import pandas as pd
import streamlit as st
from bs4 import BeautifulSoup

# ---------------------------
# Config
# ---------------------------
OPTDIR = Path("data/vectorstore/converted")
OPTDIR.mkdir(parents=True, exist_ok=True)

CSV_HEADERS = [
    "ê¸°ë³¸ì ìš”", "ì¦ë¹™ìœ í˜•", "ì¦ë¹™ì¼ì", "ìŠ¹ì¸ì‹œê°„", "ì‚¬ìš©ì¹´ë“œ",
    "í”„ë¡œì íŠ¸", "ê±°ë˜ì²˜", "ì—…ë¬´ìš©ì°¨ëŸ‰", "ê³µê¸‰ê°€ì•¡", "ë¶€ê°€ì„¸", "í•©ê³„", "ìƒì„¸ë‚´ìš©",
    "ì§€ê¸‰ìš”ì²­ì¼", "ê²°ì˜ê¸ˆì•¡", "ë¶€ì„œ", "ì‚¬ìš©ì",
]


# ---------------------------
# Helpers
# ---------------------------
def _txt(el) -> str:
    return el.get_text(strip=True) if el else ""


def decode_bytes(b: bytes) -> str:
    """Try UTF-8 first, fall back to CP949."""
    for enc in ("utf-8", "cp949", "euc-kr"):
        try:
            return b.decode(enc)
        except Exception:
            continue
    # last resort
    return b.decode("utf-8", errors="ignore")


def parse_general_info(soup: BeautifulSoup) -> Dict[str, str]:
    """
    Extract ê³µí†µ ì •ë³´: ì‚¬ìš©ì(user), ë¶€ì„œ(dept), ê²°ì˜ê¸ˆì•¡(amount), ì§€ê¸‰ìš”ì²­ì¼(req_date)
    The layout may vary slightly, so we try by class names first then by labels.
    """
    info = {"ì‚¬ìš©ì": "", "ë¶€ì„œ": "", "ê²°ì˜ê¸ˆì•¡": "", "ì§€ê¸‰ìš”ì²­ì¼": ""}

    # Try by classes seen in examples
    # ì‚¬ìš©ì
    el = soup.select_one("td.userName.BCel, td.userName")
    if el:
        info["ì‚¬ìš©ì"] = _txt(el)
    # ë¶€ì„œ
    el = soup.select_one("td.useDeptName.BCel, td.useDeptName")
    if el:
        info["ë¶€ì„œ"] = _txt(el)
    # ê²°ì˜ê¸ˆì•¡
    el = soup.select_one("td.amtTot.BCel, td.amtTot")
    if el:
        info["ê²°ì˜ê¸ˆì•¡"] = _txt(el)
    else:
        # sometimes span with id or currency widget
        el = soup.select_one("#apprLineRuleAmount, span[data-id='apprLineRuleAmount']")
        if el:
            info["ê²°ì˜ê¸ˆì•¡"] = _txt(el)
    # ì§€ê¸‰ìš”ì²­ì¼
    el = soup.select_one("td.payReqDate.BCel, td.payReqDate")
    if el:
        info["ì§€ê¸‰ìš”ì²­ì¼"] = _txt(el)

    # If any missing, try reading the summary table by labels (íšŒ ì‚¬ / ì‚¬ìš©ë¶€ì„œ / ì‚¬ìš©ì / ê²°ì˜ê¸ˆì•¡ / ì§€ê¸‰ìš”ì²­ì¼)
    if not all(info.values()):
        # Find the detailSection that contains those labels
        for table in soup.find_all("table", class_="detailSection"):
            text = table.get_text(" ", strip=True)
            if any(lbl in text for lbl in ("íšŒ ì‚¬", "ì‚¬ìš©ë¶€ì„œ", "ì‚¬ìš©ì", "ê²°ì˜ê¸ˆì•¡", "ì§€ê¸‰ìš”ì²­ì¼")):
                # grab by nearest BCel cells order used commonly:
                bcels = table.select("td.BCel")
                vals = [_txt(td) for td in bcels]
                # naive fallbacks by typical positions
                if not info["ë¶€ì„œ"] and len(vals) >= 2:
                    info["ë¶€ì„œ"] = vals[1]
                if not info["ì‚¬ìš©ì"] and len(vals) >= 3:
                    info["ì‚¬ìš©ì"] = vals[2]
                if not info["ê²°ì˜ê¸ˆì•¡"] and len(vals) >= 7:
                    info["ê²°ì˜ê¸ˆì•¡"] = vals[6]
                if not info["ì§€ê¸‰ìš”ì²­ì¼"] and len(vals) >= 8:
                    info["ì§€ê¸‰ìš”ì²­ì¼"] = vals[7]
                break

    # Final fallbacks: if any remain blank, leave as ""
    return info


def parse_line_items(soup: BeautifulSoup) -> List[Dict[str, str]]:
    """
    Extract per-line fields from #slipBplTable. Rows come in groups of 3:
      - debitTr   : ê¸°ë³¸ì ìš”, ì¦ë¹™ìœ í˜•, ì¦ë¹™ì¼ì, ìŠ¹ì¸ì‹œê°„, ì‚¬ìš©ì¹´ë“œ
      - debitTr2  : í”„ë¡œì íŠ¸, ê±°ë˜ì²˜, ì—…ë¬´ìš©ì°¨ëŸ‰, ê³µê¸‰ê°€ì•¡, ë¶€ê°€ì„¸, í•©ê³„
      - debitTr3  : ìƒì„¸ë‚´ìš©
    """
    items: List[Dict[str, str]] = []
    table = soup.find("table", {"id": "slipBplTable"})
    if not table:
        return items

    trs = table.find_all("tr", class_=["debitTr", "debitTr2", "debitTr3"])
    for i in range(0, len(trs), 3):
        group = trs[i : i + 3]
        if len(group) < 3:
            continue
        tr1, tr2, tr3 = group

        row = {
            "ê¸°ë³¸ì ìš”": _txt(tr1.select_one("td.debitRmk")),
            "ì¦ë¹™ìœ í˜•": _txt(tr1.select_one("td.taxInvestigationDivision")),
            "ì¦ë¹™ì¼ì": _txt(tr1.select_one("td.taxDate")),
            "ìŠ¹ì¸ì‹œê°„": _txt(tr1.select_one("td.apprTime")),
            "ì‚¬ìš©ì¹´ë“œ": _txt(tr1.select_one("td.cardUser")),
            "í”„ë¡œì íŠ¸": _txt(tr2.select_one("td.dzproject")),
            "ê±°ë˜ì²˜": _txt(tr2.select_one("td.dztrade")),
            "ì—…ë¬´ìš©ì°¨ëŸ‰": _txt(tr2.select_one("td.workcar")),
            "ê³µê¸‰ê°€ì•¡": _txt(tr2.select_one("td.valueOfSupply")),
            "ë¶€ê°€ì„¸": _txt(tr2.select_one("td.valueOfVAT")),
            "í•©ê³„": _txt(tr2.select_one("td.valueOfTot")),
            "ìƒì„¸ë‚´ìš©": _txt(tr3.select_one("td.summary")),
        }
        items.append(row)
    return items


def build_dataframe(items: List[Dict[str, str]], common: Dict[str, str]) -> pd.DataFrame:
    """Attach ê³µí†µê°’ to each row and return ordered DataFrame."""
    rows = []
    for it in items:
        row = {**it}
        row["ì§€ê¸‰ìš”ì²­ì¼"] = common.get("ì§€ê¸‰ìš”ì²­ì¼", "")
        row["ê²°ì˜ê¸ˆì•¡"] = common.get("ê²°ì˜ê¸ˆì•¡", "")
        row["ë¶€ì„œ"] = common.get("ë¶€ì„œ", "")
        row["ì‚¬ìš©ì"] = common.get("ì‚¬ìš©ì", "")
        rows.append(row)
    df = pd.DataFrame(rows, columns=CSV_HEADERS)
    return df


def save_csv(df: pd.DataFrame, user_name: str) -> Path:
    """Save CSV under OPTDIR with required file name pattern."""
    safe_user = (user_name or "unknown_user").strip() or "unknown_user"
    filename = f"{safe_user}_ì§€ì¶œê²°ì˜ì„œ_converted.csv"
    out_path = OPTDIR / filename
    df.to_csv(out_path, index=False, encoding="utf-8-sig")
    return out_path


# ---------------------------
# Streamlit UI
# ---------------------------
st.set_page_config(page_title="ì§€ì¶œê²°ì˜ì„œ HTML â†’ CSV ë³€í™˜ê¸°", page_icon="ğŸ“„", layout="centered")
st.title("ğŸ“„ ì§€ì¶œê²°ì˜ì„œ HTML â†’ CSV ë³€í™˜ê¸°")
st.caption("â€¢ HTMLì„ ë“œë˜ê·¸ ì•¤ ë“œëìœ¼ë¡œ ì—…ë¡œë“œí•˜ê³ , **ë³€í™˜** ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ CSVê°€ ìƒì„±ë©ë‹ˆë‹¤.\nâ€¢ ì €ì¥ ìœ„ì¹˜: `data/vectorstore/converted/`")

uploaded = st.file_uploader(
    "ì§€ì¶œê²°ì˜ì„œ HTML íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (drag & drop ì§€ì›)",
    type=["html", "htm"],
    accept_multiple_files=False,
    help="ì§€ì¶œê²°ì˜ì„œ.html íŒŒì¼ì„ ì„ íƒí•˜ê±°ë‚˜ ëŒì–´ë‹¤ ë†“ìœ¼ì„¸ìš”.",
)

convert_clicked = st.button("ë³€í™˜")

if convert_clicked:
    if not uploaded:
        st.error("íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        st.stop()

    try:
        html_text = decode_bytes(uploaded.getvalue())
        soup = BeautifulSoup(html_text, "html.parser")

        common = parse_general_info(soup)
        items = parse_line_items(soup)

        if not items:
            st.warning("í•­ëª©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. HTML êµ¬ì¡°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. (#slipBplTableì´ í•„ìš”)")
            st.json(common)
            st.stop()

        df = build_dataframe(items, common)
        out_path = save_csv(df, common.get("ì‚¬ìš©ì", ""))

        st.success(f"CSV ìƒì„± ì™„ë£Œ: {out_path}")
        st.download_button(
            label="CSV ë‹¤ìš´ë¡œë“œ",
            data=df.to_csv(index=False, encoding="utf-8-sig"),
            file_name=out_path.name,
            mime="text/csv",
        )

        with st.expander("ë¯¸ë¦¬ë³´ê¸°", expanded=True):
            st.dataframe(df, use_container_width=True)

        with st.expander("ì¶”ì¶œëœ ê³µí†µê°’"):
            st.json(common)

    except Exception as e:
        st.exception(e)
        st.error("ë³€í™˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. íŒŒì¼ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
