# services/rag.py
from __future__ import annotations
import os, glob, csv
from datetime import date
from typing import Optional, List, Tuple, Dict

import streamlit as st
from dotenv import load_dotenv
load_dotenv()

import sys, httpx
print("ğŸ§  Python ì‹¤í–‰ ê²½ë¡œ:", sys.executable)
print("ğŸ“¦ httpx ë²„ì „:", httpx.__version__)

from chromadb import PersistentClient
from chromadb.utils import embedding_functions

# ---------------- OpenAI í´ë¼ì´ì–¸íŠ¸ ----------------
try:
    from openai import OpenAI as _OpenAI
except Exception:
    _OpenAI = None

def _make_openai_client() -> Optional["_OpenAI"]:
    if not _OpenAI:
        return None
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ OPENAI_API_KEYê°€ .envì—ì„œ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    os.environ["OPENAI_API_KEY"] = api_key

    http_proxy  = os.getenv("HTTP_PROXY")  or os.getenv("http_proxy")
    https_proxy = os.getenv("HTTPS_PROXY") or os.getenv("https_proxy")

    client_args = {"timeout": 30.0}
    if http_proxy or https_proxy:
        proxies = {}
        if http_proxy:
            proxies["http://"] = http_proxy
        if https_proxy:
            proxies["https://"] = https_proxy
        client_args["proxies"] = proxies

    http_client = httpx.Client(**client_args) if client_args else None
    return _OpenAI(http_client=http_client)

_OPENAI_CLIENT = _make_openai_client()
_OPENAI_EMB_MODEL = os.getenv("EMB_MODEL", "text-embedding-3-small")

# ---------------- ì„ë² ë”© í•¨ìˆ˜ (Chroma 0.4.16+ í˜¸í™˜) ----------------
class OpenAIEmbedder:
    """ChromaDBìš© ì„ë² ë”: .name ì†ì„±ê³¼ __call__ ì œê³µ"""
    def __init__(self, client, model_name: str):
        self.client = client
        self.model_name = model_name

    @property
    def name(self) -> str:
        # âš ï¸ ë¬¸ìì—´ ë°˜í™˜, í•¨ìˆ˜ í˜¸ì¶œ ì•„ë‹˜
        return f"openai:{self.model_name}"

    # âš ï¸ Chroma 0.4.16+ì—ì„œëŠ” __call__ì´ ì •í™•íˆ inputë§Œ ë°›ì•„ì•¼ í•¨
    def __call__(self, input: list[str]) -> list[list[float]]:
        resp = self.client.embeddings.create(model=self.model_name, input=input)
        return [d.embedding for d in resp.data]

def _embedding_function():
    if _OPENAI_CLIENT:
        return OpenAIEmbedder(_OPENAI_CLIENT, _OPENAI_EMB_MODEL)
    else:
        return embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )

# ---------------- ê²½ë¡œ ì„¤ì • ----------------
DBDIR  = "data/vectorstore/chroma"
FAQDIR = "data/vectorstore"

def _chroma_client():
    os.makedirs(DBDIR, exist_ok=True)
    return PersistentClient(path=DBDIR)

# ---------------- íŒŒì¼ ë¦¬ë” ----------------
def _read(path: str) -> str:
    if path.lower().endswith((".md", ".txt")):
        return open(path, "r", encoding="utf-8").read()
    try:
        from pypdf import PdfReader
        return "\n".join(page.extract_text() or "" for page in PdfReader(path).pages)
    except Exception:
        return ""

def _read_qa_csv(path: str) -> list[tuple[str, str]]:
    rows = []
    with open(path, encoding="utf-8") as f:
        for r in csv.DictReader(f):
            q = (r.get("question") or "").strip()
            a = (r.get("answer") or "").strip()
            if q and a:
                rows.append((q, a))
    return rows

def _read_qa_md(path: str) -> list[tuple[str, str]]:
    txt = open(path, "r", encoding="utf-8").read()
    pairs = []
    for block in txt.split("### Q:")[1:]:
        q, _, rest = block.partition("\n")
        a_tag = "A:"
        a = rest.split("\n", 1)[0] if a_tag not in rest else rest.split(a_tag, 1)[1].strip()
        q, a = q.strip(), a.strip()
        if q and a:
            pairs.append((q, a))
    return pairs

# ---------------- ì¸ë±ìŠ¤ ë¹Œë“œ ----------------
def build_index() -> None:
    os.makedirs(DBDIR, exist_ok=True)
    cli = _chroma_client()
    col = cli.get_or_create_collection("faqs_openai", embedding_function=_embedding_function())

    # 1) PDF/MD/TXT ë¬¸ì„œ ìƒ‰ì¸
    files = glob.glob(f"{FAQDIR}/*.md") + glob.glob(f"{FAQDIR}/*.txt") + glob.glob(f"{FAQDIR}/*.pdf")
    ids, docs, metas = [], [], []
    for f in files:
        txt = _read(f)
        if not txt.strip():
            continue
        ids.append(os.path.basename(f))
        docs.append(txt[:10000])
        metas.append({"path": f})
    if ids:
        col.upsert(ids=ids, documents=docs, metadatas=metas)

    # 2) CSV/MD Q&A ìƒ‰ì¸
    qa_files = glob.glob(f"{FAQDIR}/*.csv") + glob.glob(f"{FAQDIR}/faq_qa.md") + glob.glob(f"{FAQDIR}/*_qa.md")
    q_ids, q_docs, q_metas = [], [], []
    for f in qa_files:
        pairs = _read_qa_csv(f) if f.endswith(".csv") else _read_qa_md(f)
        for idx, (q, a) in enumerate(pairs):
            q_ids.append(f"{os.path.basename(f)}::Q{idx+1}")
            q_docs.append(f"[Q] {q}\n[A] {a}")
            q_metas.append({"path": f, "type": "qa"})
    if q_ids:
        col.upsert(ids=q_ids, documents=q_docs, metadatas=q_metas)

    cli.persist()

# ---------------- ì§ˆì˜ ì‘ë‹µ ----------------
def rag_answer(question: str, k: int = 3) -> tuple[str, list[dict]]:
    cli = PersistentClient(path=DBDIR)
    col = cli.get_or_create_collection("faqs_openai", embedding_function=_embedding_function())
    r = col.query(query_texts=[question], n_results=k)
    ctx = "\n\n".join(r["documents"][0]) if r and r.get("documents") else ""

    if _OPENAI_CLIENT is not None and ctx.strip():
        msgs = [
            {"role": "system", "content": "ì‚¬ë‚´ ê·œì •/FAQë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°íˆ í•œêµ­ì–´ë¡œ ë‹µí•˜ì„¸ìš”. ì¸ìš©ì€ ë”°ì˜´í‘œë¡œ í‘œì‹œ."},
            {"role": "user",   "content": f"[ì»¨í…ìŠ¤íŠ¸]\n{ctx}\n\n[ì§ˆë¬¸]\n{question}"},
        ]
        try:
            a = _OPENAI_CLIENT.chat.completions.create(
                model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
                messages=msgs,
                temperature=0.1,
            )
            return a.choices[0].message.content, (r["metadatas"][0] if r else [])
        except Exception as e:
            print("âŒ LLM ì˜¤ë¥˜:", e)

    return (f"ìƒìœ„ ìŠ¤ë‹ˆí«:\n{ctx[:1200]}\n\n(OPENAI_API_KEY ì—†ê±°ë‚˜ ì˜¤ë¥˜ ì‹œ ìŠ¤ë‹ˆí«ë§Œ í‘œì‹œ)",
            r["metadatas"][0] if r else [])

# ---------------- ê¸°ë³¸ UI ----------------
st.title("í…ìŠ¤íŠ¸ ì…ë ¥ ì‹œ rag_answer() ìˆ˜í–‰")
user_input = st.text_input("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")

if user_input:
    st.write(f"ì‚¬ìš©ìê°€ ì…ë ¥í•œ í…ìŠ¤íŠ¸: {user_input}")

    # PDF/MD/TXT ìƒ‰ì¸ í›„ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    try:
        build_index()
        cli = PersistentClient(path=DBDIR)
        col = cli.get_or_create_collection("faqs_openai")
        st.write("true" if col.count() > 0 else "false")
    except Exception as e:
        st.write("false")
        print("âŒ ë¹Œë“œ ì˜¤ë¥˜:", e)

    # ì§ˆë¬¸ì— ëŒ€í•œ RAG ë‹µë³€
    answer, metadata = rag_answer(user_input)
    st.markdown("### RAG ë‹µë³€")
    st.write(answer)
